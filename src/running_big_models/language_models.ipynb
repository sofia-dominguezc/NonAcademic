{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d5ed20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "import pandas as pd\n",
    "load_dotenv()\n",
    "\n",
    "hugkey = os.getenv('HUGGINGFACE_KEY_LLAMA')\n",
    "login(hugkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98271fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, device_map='auto')\n",
    "# tokenizer.pad_token is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7226c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', load_in_8bit=True)\n",
    "# model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map='auto')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5bde5d",
   "metadata": {},
   "source": [
    "This is my current draft (see interactive_chat.py)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35c6eb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation.streamers import BaseStreamer\n",
    "import time\n",
    "\n",
    "class CallbackStreamer(BaseStreamer):\n",
    "    def __init__(self, tokenizer):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def put(self, value):\n",
    "        decoded = self.tokenizer.batch_decode(value, skip_special_tokens=True)\n",
    "        import pdb; pdb.set_trace()\n",
    "        print(decoded[0], end=\"\")\n",
    "        # time.sleep(0.01)\n",
    "\n",
    "    def end(self):\n",
    "        print()\n",
    "\n",
    "streamer = CallbackStreamer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52e229ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32mc:\\users\\sofia\\appdata\\local\\temp\\ipykernel_21256\\2497998121.py\u001b[0m(12)\u001b[0;36mput\u001b[1;34m()\u001b[0m\n",
      "\n",
      "tensor([[128000, 128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,\n",
      "           2696,     25,   6790,    220,   2366,     18,    198,  15724,   2696,\n",
      "             25,    220,   2705,  10263,    220,   2366,     20,    271,   2675,\n",
      "           2351,    264,  11190,   6369,   6465,    430,  11503,   4860,    323,\n",
      "          68577,    279,   9256,    499,   2351,   2728,     13,   1472,   2351,\n",
      "           2167,    323,   1193,   9407,    389,    264,   8712,    422,    420,\n",
      "            374,  11472,    555,    279,   1217,     13,   1472,   1193,   3493,\n",
      "           4495,   2038,    323,   2019,    433,    994,    499,   1541,    956,\n",
      "           1440,    279,   4320,     13, 128009, 128006,    882, 128007,    271,\n",
      "          13347,      0,   3053,    499,   3371,    757,   1148,    374,    279,\n",
      "           6864,    315,   9822,     30,   7429,   3371,    757,    922,   1268,\n",
      "           1917,   4208,    220,     17,   3940,     13, 128009, 128006,  78191,\n",
      "         128007,    271]])\n",
      "[\"system\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 06 Jul 2025\\n\\nYou're a helpful chatbot that answers questions and solves the tasks you're given. You're direct and only expand on a topic if this is requested by the user. You only provide correct information and say it when you don't know the answer.user\\n\\nHi! Can you tell me what is the capital of France? Also tell me about how world war 2 started.assistant\\n\\n\"]\n",
      "\"system\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 06 Jul 2025\\n\\nYou're a helpful chatbot that answers questions and solves the tasks you're given. You're direct and only expand on a topic if this is requested by the user. You only provide correct information and say it when you don't know the answer.user\\n\\nHi! Can you tell me what is the capital of France? Also tell me about how world war 2 started.assistant\\n\\n\"\n",
      "1\n",
      "'\\n'\n"
     ]
    }
   ],
   "source": [
    "# prompt = [\"What is the capital of France?\"]\n",
    "chat = [\n",
    "    {'role': 'system', 'content': \"You're a helpful chatbot that answers questions and solves the tasks you're given. You're direct and only expand on a topic if this is requested by the user. You only provide correct information and say it when you don't know the answer.\"},\n",
    "    {'role': 'user', 'content': \"Hi! Can you tell me what is the capital of France? Also tell me about how world war 2 started.\"},\n",
    "]\n",
    "chat_text = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "# print(chat_text)\n",
    "input_ids = tokenizer(chat_text, padding=True, return_tensors=\"pt\").to('cuda')\n",
    "outputs = model.generate(**input_ids, do_sample=True, max_length=250, streamer=streamer)\n",
    "tokenizer.decode(outputs[0][input_ids['input_ids'].shape[-1]:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e734a6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce9525e36a04859b3e50bf79375f30d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You're chatting with meta-llama/LLama-3.2-3B-Instruct. Press q to exit.\n",
      "\n",
      "\u001b[0;32m<llm>\u001b[0;0m Hi Sofia! How can I help you today?\n",
      "\n",
      "\n",
      "\u001b[0;32m<llm>\u001b[0;0m What subject and what's the specific question or task you're struggling with? I'll do my best to assist you.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers.generation.streamers import BaseStreamer\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "class CallbackStreamer(BaseStreamer):\n",
    "    def __init__(self, tokenizer):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def put(self, value):\n",
    "        decoded = self.tokenizer.batch_decode(value, skip_special_tokens=True)[0]\n",
    "        breakpoint()\n",
    "        if len(decoded) > 50:\n",
    "            return\n",
    "        print(decoded, end=\"\")\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    def end(self):\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "def main(model_name, quantize=True):\n",
    "    \"\"\"\n",
    "    Support an interactive chat with the model in the terminal.\n",
    "    \"\"\"\n",
    "    bnb_config = BitsAndBytesConfig(load_in_8bit=quantize)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, device_map='auto', quantization_config=bnb_config, \n",
    "    )\n",
    "    model.eval()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, device_map='auto')\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        model.generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "    streamer = CallbackStreamer(tokenizer)\n",
    "\n",
    "    chat = [\n",
    "        {'role': 'system', 'content': \"You're a helpful chatbot that answers questions and solves the tasks you're given. You're direct and only expand on a topic if this is requested by the user. You only provide correct information and say it when you don't know the answer.\"},\n",
    "        {'role': 'assistant', 'content': \"Hi Sofia! How can I help you today?\"},\n",
    "    ]\n",
    "    print(f\"\\nYou're chatting with {model_name}. Press q to exit.\")\n",
    "    print(\"\\n\\033[0;32m<llm>\\033[0;0m \" + chat[-1]['content'] + \"\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"\\033[0;32m<user>\\033[0;0m \")\n",
    "        if user_input == \"q\":\n",
    "            break\n",
    "        chat.append({'role': 'user', 'content': user_input})\n",
    "        chat_text = tokenizer.apply_chat_template(\n",
    "            chat, tokenize=False, add_generation_prompt=True,\n",
    "        )\n",
    "        print(\"\\n\\033[0;32m<llm>\\033[0;0m \", end=\"\")\n",
    "        inputs = tokenizer(chat_text, padding=True, return_tensors='pt').to('cuda')\n",
    "        output = model.generate(\n",
    "            **inputs, streamer=streamer,\n",
    "            do_sample=True,\n",
    "            max_new_tokens=512,\n",
    "        )\n",
    "        response = tokenizer.decode(\n",
    "            output[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True,\n",
    "        )[0]\n",
    "        chat.append({'role': 'assistant', 'content': response})\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "hugkey = os.getenv('HUGGINGFACE_KEY_LLAMA')\n",
    "login(hugkey)\n",
    "\n",
    "model_name = \"meta-llama/LLama-3.2-3B-Instruct\"\n",
    "main(model_name, quantize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8193f323",
   "metadata": {},
   "source": [
    "Here is my first draft of the code\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13bd118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability(logits, token, tokenizer):\n",
    "    \"\"\"Calculate the probability of the token given the logits\"\"\"\n",
    "    assert token in tokenizer.vocab, \"token not in tokenizer vocabulary\"\n",
    "    probs = F.softmax(logits.logits, dim=-1)  # (B, length, vocab)\n",
    "    return probs[:, -1, tokenizer.vocab[token]]\n",
    "\n",
    "\n",
    "def llm_forward(model, tokens):\n",
    "    \"\"\"\n",
    "    A forward pass through the model.\n",
    "\n",
    "    Args:\n",
    "        model: Huggingface model.\n",
    "        tokens (dic): contains input_ids and attention_mask of (num_sen, len_sen)\n",
    "    \n",
    "    Returns:\n",
    "        probs (tensor): a (num_sen, vocab) probability distribution for next token\n",
    "    \"\"\"\n",
    "    # Preliminares: idx for sentences and for padding start\n",
    "    num_sen, _ = tokens['input_ids'].shape\n",
    "    i = torch.arange(0, num_sen, device='cuda')               # (num_sen, )\n",
    "    mask_limit = torch.sum(tokens['attention_mask'], dim=-1)  # (num_sen, )\n",
    "\n",
    "    # Run the model and extract probabilities for next token\n",
    "    with torch.no_grad():\n",
    "        logits = model(**tokens).logits          # (num_sen, len_sen, vocab)\n",
    "        probs = F.softmax(logits[i, mask_limit - 1], dim=-1)  # (num_sen, vocab)\n",
    "    return probs\n",
    "\n",
    "\n",
    "def autoregressive_forward_topk(model, tokenizer, sentences, k=1):\n",
    "    \"\"\"\n",
    "    Does a single pass of the model and outputs the top k predictions.\n",
    "    Organizes the ids, tokens, and probabilities in a dataframe\n",
    "    \"\"\"\n",
    "    # Calculate top predictions\n",
    "    tokens = tokenizer(sentences, padding=True, return_tensors='pt').to('cuda')\n",
    "    probs = llm_forward(model, tokens)\n",
    "    topk_probs, topk_ids = torch.topk(probs, k, dim=-1)  # (num_sen, k)\n",
    "    topk_tokens = [tokenizer.batch_decode(sentence_toks) for sentence_toks in topk_ids]\n",
    "\n",
    "    # Construct dataframe\n",
    "    data = []\n",
    "    num_sentences = topk_ids.shape[0]\n",
    "    for sent_idx in range(num_sentences):\n",
    "        for guess_idx in range(k):\n",
    "            data.append({\n",
    "                'sentence': str(sent_idx) + \": \" + sentences[sent_idx],\n",
    "                'num_guess': guess_idx,\n",
    "                'token': topk_tokens[sent_idx][guess_idx],\n",
    "                'token_id': topk_ids[sent_idx][guess_idx].item(),\n",
    "                'probability': topk_probs[sent_idx][guess_idx].item()\n",
    "            })\n",
    "    df = pd.DataFrame(data).set_index(['sentence', 'num_guess'])\n",
    "    df = df.sort_index(axis=0, ascending=[True, True])\n",
    "    return df\n",
    "\n",
    "\n",
    "def llm_single_pass(model, tokens, k=1):\n",
    "    \"\"\"\n",
    "    Does a single pass of the model and chooses randomly from\n",
    "        the top k options to continue the text.\n",
    "\n",
    "    Args:\n",
    "        model: HuggingFace model\n",
    "        tokens (token item): input_ids has shape (num_sen, len_sen)\n",
    "        k (int, optional): number of hypothesis to consider\n",
    "\n",
    "    Returns:\n",
    "        pred_tokens: tensor of shape (num_sen, ) with tokens to add\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    probs = llm_forward(model, tokens)\n",
    "    topk_probs, topk_tokens = torch.topk(probs, k, dim=-1)  # (num_sen, k)\n",
    "\n",
    "    # Choose element at random\n",
    "    pred_tokens = torch.empty((probs.shape[0], ), dtype=torch.int64, device='cuda')\n",
    "    for i, (sen_prob, sen_tokens) in enumerate(zip(topk_probs, topk_tokens)):\n",
    "        pred_id = random.choices(range(k), weights=sen_prob)\n",
    "        pred_tokens[i] = sen_tokens[pred_id]\n",
    "    return pred_tokens\n",
    "\n",
    "\n",
    "def llm_pipeline(model, tokenizer, sentences, k=1, max_tokens=32):\n",
    "    \"\"\"\n",
    "    Iteratively complete each sentence using the specified model.\n",
    "        Each step randomly chooses one of the top k options.\n",
    "\n",
    "    Args:\n",
    "        model, HuggingFace model\n",
    "        tokenizer, HuggingFace tokenizer\n",
    "        sentences: List[str], list of sentences to give the model\n",
    "        k: Optional[int], number of hypothesis to consider\n",
    "        max_tokens: Optional[int], number of tokens to add to each sentence\n",
    "\n",
    "    Returns:\n",
    "        sentences: list of completed sentences\n",
    "    \"\"\"\n",
    "    # Create tokens of shape (num_sen, len_sen)\n",
    "    tokens = tokenizer(sentences, padding=True, return_tensors='pt').to('cuda')\n",
    "\n",
    "    # Define idx for sentences and idx where the masking starts\n",
    "    i = torch.arange(0, len(sentences), device='cuda')        # (num_sen, )\n",
    "    mask_limit = torch.sum(tokens['attention_mask'], dim=-1)  # (num_sen, )\n",
    "    extra_padding = torch.full(\n",
    "        size=(len(sentences), 1),\n",
    "        fill_value=tokenizer.vocab[tokenizer.special_tokens_map['pad_token']],\n",
    "        device='cuda',\n",
    "    )\n",
    "\n",
    "    # Main loop: use model repeteadly and iterate over tokens\n",
    "    for _ in range(max_tokens):\n",
    "        # NOTE: Simplification: all mask limits are incremented in each step\n",
    "        new_tokens = llm_single_pass(model, tokens, k)            # (num_sen, )\n",
    "        token_ids = torch.cat((tokens['input_ids'], extra_padding.clone()), dim=-1)\n",
    "        attn_mask = torch.cat((tokens['attention_mask'], torch.zeros_like(extra_padding)), dim=-1)\n",
    "\n",
    "        token_ids[i, mask_limit] = new_tokens\n",
    "        attn_mask[i, mask_limit] = 1\n",
    "        mask_limit += 1\n",
    "\n",
    "        tokens = {'input_ids': token_ids, 'attention_mask': attn_mask}\n",
    "\n",
    "    # Convert back to sentences\n",
    "    sentences = [\n",
    "        tokenizer.decode(t, skip_special_tokens=True) for t in tokens['input_ids']\n",
    "    ]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30cb26eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>token_id</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence</th>\n",
       "      <th>num_guess</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0: Most people wouldn't bat an eye if</th>\n",
       "      <th>0</th>\n",
       "      <td>they</td>\n",
       "      <td>814</td>\n",
       "      <td>0.389648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you</td>\n",
       "      <td>499</td>\n",
       "      <td>0.247681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>264</td>\n",
       "      <td>0.076721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>their</td>\n",
       "      <td>872</td>\n",
       "      <td>0.072083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>someone</td>\n",
       "      <td>4423</td>\n",
       "      <td>0.051910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1: Pasta is a popular dish in many</th>\n",
       "      <th>0</th>\n",
       "      <td>cultures</td>\n",
       "      <td>27833</td>\n",
       "      <td>0.317139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>countries</td>\n",
       "      <td>5961</td>\n",
       "      <td>0.195435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>parts</td>\n",
       "      <td>5596</td>\n",
       "      <td>0.177856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Italian</td>\n",
       "      <td>15155</td>\n",
       "      <td>0.060516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>households</td>\n",
       "      <td>29939</td>\n",
       "      <td>0.054260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2: El salvador is well known because</th>\n",
       "      <th>0</th>\n",
       "      <td>of</td>\n",
       "      <td>315</td>\n",
       "      <td>0.787109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it</td>\n",
       "      <td>433</td>\n",
       "      <td>0.095459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the</td>\n",
       "      <td>279</td>\n",
       "      <td>0.023041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>its</td>\n",
       "      <td>1202</td>\n",
       "      <td>0.020020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>they</td>\n",
       "      <td>814</td>\n",
       "      <td>0.007656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       token  token_id  \\\n",
       "sentence                              num_guess                          \n",
       "0: Most people wouldn't bat an eye if 0                 they       814   \n",
       "                                      1                  you       499   \n",
       "                                      2                    a       264   \n",
       "                                      3                their       872   \n",
       "                                      4              someone      4423   \n",
       "1: Pasta is a popular dish in many    0             cultures     27833   \n",
       "                                      1            countries      5961   \n",
       "                                      2                parts      5596   \n",
       "                                      3              Italian     15155   \n",
       "                                      4           households     29939   \n",
       "2: El salvador is well known because  0                   of       315   \n",
       "                                      1                   it       433   \n",
       "                                      2                  the       279   \n",
       "                                      3                  its      1202   \n",
       "                                      4                 they       814   \n",
       "\n",
       "                                                 probability  \n",
       "sentence                              num_guess               \n",
       "0: Most people wouldn't bat an eye if 0             0.389648  \n",
       "                                      1             0.247681  \n",
       "                                      2             0.076721  \n",
       "                                      3             0.072083  \n",
       "                                      4             0.051910  \n",
       "1: Pasta is a popular dish in many    0             0.317139  \n",
       "                                      1             0.195435  \n",
       "                                      2             0.177856  \n",
       "                                      3             0.060516  \n",
       "                                      4             0.054260  \n",
       "2: El salvador is well known because  0             0.787109  \n",
       "                                      1             0.095459  \n",
       "                                      2             0.023041  \n",
       "                                      3             0.020020  \n",
       "                                      4             0.007656  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chats = [\n",
    "    \"Most people wouldn't bat an eye if\",\n",
    "    \"Pasta is a popular dish in many\",\n",
    "    \"El salvador is well known because\",\n",
    "]\n",
    "df = autoregressive_forward_topk(model, tokenizer, chats, k=5)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86525165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.3842e-07, 2.3842e-07, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [1.7881e-07, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [3.3975e-06, 8.3447e-07, 5.9605e-08,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(chats, padding=True, return_tensors='pt').to('cuda')\n",
    "llm_forward(model, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ae982b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most people wouldn't bat an eye if a stranger asked if they could borrow their phone to call for an ambulance. But when that stranger is a police officer, the answer may come with some hesitation. That's because a new bill introduced in the House of Commons on Thursday aims to give police officers the power to seize your phone in an emergency.\n",
      "The proposal,\n",
      "\n",
      "\n",
      "\n",
      "Pasta is a popular dish in many households, both for lunch and for dinner. However, the preparation of pasta often requires the use of a lot of cooking oil. This can lead to an increase in your daily calorie consumption, which can be counter-productive if you are trying to lose weight. Fortunately, there is a way to reduce pasta’s calorie count\n",
      "\n",
      "\n",
      "\n",
      "El salvador is well known because of the coffee. It is a popular commodity for the coffee lovers worldwide. It is a country which produces and exports coffee in great volumes. It is a well known coffee bean and it is very famous in the whole world.\n",
      "El Salvador is well known as a coffee country. El Salvador has a very rich history and it\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = llm_pipeline(model, tokenizer, chats, k=10, max_tokens=64)\n",
    "for sen in outputs:\n",
    "    print(sen + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cf9961",
   "metadata": {},
   "source": [
    "Terminal output\n",
    "\n",
    "---\n",
    "\n",
    "The idea is to format the python text after it's been written, and override the plain text there was before.\n",
    "\n",
    "Or maybe not showing the code as plain text and wait for it to finish.\n",
    "\n",
    "Indentation problems don't happen in the terminal.\n",
    "\n",
    "Render it each time a new \\n is written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c1a482d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python functions are defined with the def keyword.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Here's a code snippet:\n",
       "\n",
       "<span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">def</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #a6e22e; text-decoration-color: #a6e22e; background-color: #272822\">foo</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">(x):</span><span style=\"background-color: #272822\">                                                                                                        </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    print(x)</span><span style=\"background-color: #272822\">                                                                                                       </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Here's a code snippet:\n",
       "\n",
       "\u001b[38;2;102;217;239;48;2;39;40;34mdef\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;166;226;46;48;2;39;40;34mfoo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mx\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                                                                                        \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mx\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                       \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.console import Console, Group\n",
    "from rich.syntax import Syntax\n",
    "\n",
    "console = Console()\n",
    "\n",
    "code = \"\"\"def foo(x):\\n\\tprint(x)\"\"\"\n",
    "# code = \"\"\"a = \\sum_{n=0}^\\infty x^n\"\"\"\n",
    "syntax = Syntax(code, \"python\", theme='monokai', line_numbers=False)\n",
    "group = Group(\"Here's a code snippet:\\n\", syntax)\n",
    "\n",
    "print(\"Python functions are defined with the def keyword.\")\n",
    "console.print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27716421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42333\n",
      "55375\n",
      "74694\n"
     ]
    }
   ],
   "source": [
    "for i in range(128000):\n",
    "    token = tokenizer.decode([i])\n",
    "    # if \"\\n\" in token and token[-1] != \"\\n\":\n",
    "        # print(\"#\" + token + \"*\")\n",
    "    # if \"\\t\" in token:\n",
    "    if \"```\" in token:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b78956e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 58040)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max((len(tok), i) for tok, i in tokenizer.vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28ba4410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ```\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([42333])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
